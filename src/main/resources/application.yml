spring:
  application:
    name: databaseevent
  jpa:
    database-platform: org.hibernate.dialect.H2Dialect
    database: H2
    show_sql: true
#    generate-ddl: true
    properties:
      hibernate:
        format_sql: true # To beautify or pretty print the SQL
#        ddl-auto: validate # not to overwrite my schema (if you had schema.sql scripts). Usually hibernate looks at the entities and generates schema based on those entities
        ddl-auto: create
  datasource:
    url: jdbc:h2:mem:mydb
    username: sa
    password: password
    driverClassName: org.h2.Driver
  h2:
    console:
      enabled: true
      path: /h2-console # URL to access this console http://localhost:9006/databaseevent/h2-console. Change the JDBC URL to whats above and login with sa/password
      settings:
        trace: false
        web-allow-others: false
  cloud:
    stream:
      bindings:
        process-in-0:
          contentType: application/*+avro
          destination: customer-inbound-topic
          group: customer-inbound-group
          consumer:
            useNativeDecoding: true
        inbound-in-0:
          contentType: application/*+avro
          destination: customer-inbound-topic
          group: customer-inbound-only-group
          consumer:
            useNativeDecoding: true
        process-out-0:
          contentType: application/*+avro
          destination: customer-outbound-topic
          producer:
            useNativeDecoding: true
        output:
          contentType: application/*+avro
          destination: customer-outbound-topic
          binder: kafka
          producer:
            useNativeEncoding: true
            configuration:
              value.serializer: org.springframework.cloud.stream.schema.avro.AvroSchemaRegistryClientMessageConverter
              schema.registry.url: http://localhost:8081

        bindings:
          output:
            producer:
              configuration:
                value.serializer: io.confluent.kafka.serializers.KafkaAvroSerializer
                schema.registry.url: http://localhost:8081

      kafka:
        binder:
          brokers: localhost:9092
          autoCreateTopics: false
#          producerProperties:
#            value.subject.name.strategy: io.confluent.kafka.serializers.subject.RecordNameStrategy
        bindings:
          process-in-0:
              ackMode: MANUAL
        streams:
          binder:
            configuration:
#              default:
#                key:
#                  serde: org.apache.kafka.common.serialization.Serdes$StringSerde
#                value:
#                  serde: io.confluent.kafka.streams.serdes.avro.SpecificAvroSerde
              schema.registry.url: http://localhost:8081
          bindings:
            inbound-in-0:
              consumer:
                applicationId: customer-inbound-only-group
                keySerde: org.apache.kafka.common.serialization.Serdes$StringSerde
                valueSerde: io.confluent.kafka.streams.serdes.avro.SpecificAvroSerde
            process-in-0:
              consumer:
                applicationId: customer-inbound-group
                keySerde: org.apache.kafka.common.serialization.Serdes$StringSerde
                valueSerde: io.confluent.kafka.streams.serdes.avro.SpecificAvroSerde
            process-out-0:
              producer:
                contentType: application/*+avro
                keySerde: org.apache.kafka.common.serialization.Serdes$StringSerde
                valueSerde: io.confluent.kafka.streams.serdes.avro.SpecificAvroSerde
#            output:
#              producer:
#                keySerde: org.apache.kafka.common.serialization.Serdes$StringSerde
#                valueSerde: io.confluent.kafka.streams.serdes.avro.SpecificAvroSerde

application:
  name: databaseevent

server:
  port: 9006
  servlet:
    context-path: /databaseevent

management:
  endpoint:
    health:
      show-details: ALWAYS
  endpoints:
    web:
      base-path: /
